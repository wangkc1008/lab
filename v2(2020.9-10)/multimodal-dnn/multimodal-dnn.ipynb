{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 310)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/wangkc/Desktop/胡喜风预测论文/mixed_data_0717_drop_replace_fill_100_5079.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', '1', '2', '3', '4', '5', '6_x', '7', '8', '9', '10',\n",
       "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21',\n",
       "       '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
       "       '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43',\n",
       "       '44', '45', '46', '47', '48', '49', '50', '51_x', '52', '53', '54',\n",
       "       '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65',\n",
       "       '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76',\n",
       "       '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87_x',\n",
       "       '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98',\n",
       "       '99', '100', '101', '102', '103', '104', '105', '106', '107',\n",
       "       '108', '109', '110', '111', '112', '113', '114', '115', '116_x',\n",
       "       '117', '118', '119', '120', '121', '122', '123', '124', '125',\n",
       "       '126', '127', '128', '129', '130', '131', '132', '133', '134',\n",
       "       '135', '136', '137', '138', '139', '140', '141', '142', '143',\n",
       "       '144', '145', '146', '147', '148', '149', '150', '151', '152',\n",
       "       '153', '154', '155', '156', '157', '158', '159', '160', '161',\n",
       "       '162', '163', '164', '165', '166', '167', '168', '169', '170',\n",
       "       '171', '172', '173', '174', '175', '176', '177', '178', '179',\n",
       "       '180', '181', '182', '183', '184', '185', '186', '187', '188',\n",
       "       '189', '190', '191', '192', '193', '194', '195', '196', '197',\n",
       "       '198_x', '199', '200', 'Unnamed: 0.1', 'discharge_location',\n",
       "       'religion', 'marital_status', 'ethnicity', 'diagnosis', 'gender',\n",
       "       'descriptin', '644', '617', '8555', '220046', '220545', '618',\n",
       "       '430', '220546', '220603', '220624', '220645', '220050', '1522',\n",
       "       '220051', '220180', '220210', '220228', '212', '431', '432',\n",
       "       '1523', '1524', '1526', '1529', '455', '762', '470', '51_y',\n",
       "       '1535', '87_y', '1536', '1540', '1542', '198_y', '116_y', '781',\n",
       "       '8441', '8368', '225309', '8480', '225310', '225693', '226534',\n",
       "       '226537', '226540', '227429', '227457', '786', '787', '789', '793',\n",
       "       '806', '813', '814', '824', '827', '829', '833', '837', '850',\n",
       "       '851', '853', '861', '1087', '1127', '51249', '51274', '51277',\n",
       "       '51279', '51301', '50931', '50861', '50862', '50863', '50868',\n",
       "       '50878', '50882', '50893', '50912', '50954', '51006', '51221',\n",
       "       '51222', '51237', '50970', '51464', '51491', '50910', '51244',\n",
       "       '51256', '50911', '50963', '50909', '50903', '50904', '50905',\n",
       "       '50907', '51000', '50906', 'age', 'flag'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '3', '4', '5', '6_x', '7', '8', '9', '10', '11', '12',\n",
       "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',\n",
       "       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
       "       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
       "       '46', '47', '48', '49', '50', '51_x', '52', '53', '54', '55', '56',\n",
       "       '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67',\n",
       "       '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78',\n",
       "       '79', '80', '81', '82', '83', '84', '85', '86', '87_x', '88', '89',\n",
       "       '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100',\n",
       "       '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
       "       '110', '111', '112', '113', '114', '115', '116_x', '117', '118',\n",
       "       '119', '120', '121', '122', '123', '124', '125', '126', '127',\n",
       "       '128', '129', '130', '131', '132', '133', '134', '135', '136',\n",
       "       '137', '138', '139', '140', '141', '142', '143', '144', '145',\n",
       "       '146', '147', '148', '149', '150', '151', '152', '153', '154',\n",
       "       '155', '156', '157', '158', '159', '160', '161', '162', '163',\n",
       "       '164', '165', '166', '167', '168', '169', '170', '171', '172',\n",
       "       '173', '174', '175', '176', '177', '178', '179', '180', '181',\n",
       "       '182', '183', '184', '185', '186', '187', '188', '189', '190',\n",
       "       '191', '192', '193', '194', '195', '196', '197', '198_x', '199',\n",
       "       '200', 'discharge_location', 'religion', 'marital_status',\n",
       "       'ethnicity', 'diagnosis', 'gender', 'descriptin', '644', '617',\n",
       "       '8555', '220046', '220545', '618', '430', '220546', '220603',\n",
       "       '220624', '220645', '220050', '1522', '220051', '220180', '220210',\n",
       "       '220228', '212', '431', '432', '1523', '1524', '1526', '1529',\n",
       "       '455', '762', '470', '51_y', '1535', '87_y', '1536', '1540',\n",
       "       '1542', '198_y', '116_y', '781', '8441', '8368', '225309', '8480',\n",
       "       '225310', '225693', '226534', '226537', '226540', '227429',\n",
       "       '227457', '786', '787', '789', '793', '806', '813', '814', '824',\n",
       "       '827', '829', '833', '837', '850', '851', '853', '861', '1087',\n",
       "       '1127', '51249', '51274', '51277', '51279', '51301', '50931',\n",
       "       '50861', '50862', '50863', '50868', '50878', '50882', '50893',\n",
       "       '50912', '50954', '51006', '51221', '51222', '51237', '50970',\n",
       "       '51464', '51491', '50910', '51244', '51256', '50911', '50963',\n",
       "       '50909', '50903', '50904', '50905', '50907', '51000', '50906',\n",
       "       'age', 'flag'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(labels=['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 308)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure, data_unstructure = data.iloc[:, 0:200], data.iloc[:, 200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 108)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unstructure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = data_unstructure.corr()['flag'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.44188461,  0.34834065,  0.31095106,  0.29790541,\n",
       "        0.27268167,  0.2718151 ,  0.22905935,  0.22840715,  0.21215508,\n",
       "        0.20216547,  0.19953281,  0.18619697,  0.18439669,  0.16992431,\n",
       "        0.16440352,  0.16432356,  0.15335323,  0.14109696,  0.13949691,\n",
       "        0.12774348,  0.1271463 ,  0.11838982,  0.11299193,  0.1125567 ,\n",
       "        0.10861225,  0.1071288 ,  0.10025498,  0.09863866,  0.09458484,\n",
       "        0.09294859,  0.09262481,  0.09137232,  0.08903374,  0.08362927,\n",
       "        0.07543028,  0.07361162,  0.07360355,  0.0647498 ,  0.0627509 ,\n",
       "        0.05857573,  0.05222199,  0.05180129,  0.05133955,  0.04385232,\n",
       "        0.04211161,  0.03680885,  0.03597658,  0.03231726,  0.03117802,\n",
       "        0.02990743,  0.02118805,  0.01793353,  0.01481552,  0.00980294,\n",
       "        0.00526859,  0.0040515 ,  0.00249607,  0.00131689,  0.00101453,\n",
       "       -0.00442209, -0.00652561, -0.01491198, -0.0226915 , -0.0240744 ,\n",
       "       -0.02448223, -0.02635538, -0.02834546, -0.02946103, -0.03165345,\n",
       "       -0.03171733, -0.03414896, -0.03753092, -0.03795563, -0.03961062,\n",
       "       -0.04078438, -0.04597013, -0.05368642, -0.05778557, -0.05944319,\n",
       "       -0.06300886, -0.06446793, -0.06503558, -0.06854764, -0.06934461,\n",
       "       -0.07469666, -0.07549279, -0.08066237, -0.08301098, -0.08772217,\n",
       "       -0.09448277, -0.09965595, -0.10020189, -0.10905291, -0.11073511,\n",
       "       -0.11316227, -0.11405799, -0.12465224, -0.13460134, -0.13540862,\n",
       "       -0.15127792, -0.15433649, -0.16713406, -0.21699047, -0.23124315,\n",
       "       -0.25543088, -0.26503579, -0.32656055])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unstructure_corr = data_unstructure[corr_data[(corr_data > 0.01) | (corr_data < -0.01)].index]\n",
    "data_unstructure_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>51006</th>\n",
       "      <th>51277</th>\n",
       "      <th>781</th>\n",
       "      <th>age</th>\n",
       "      <th>50868</th>\n",
       "      <th>644</th>\n",
       "      <th>50912</th>\n",
       "      <th>432</th>\n",
       "      <th>...</th>\n",
       "      <th>787</th>\n",
       "      <th>51279</th>\n",
       "      <th>50893</th>\n",
       "      <th>51244</th>\n",
       "      <th>51222</th>\n",
       "      <th>50882</th>\n",
       "      <th>51249</th>\n",
       "      <th>198_y</th>\n",
       "      <th>50862</th>\n",
       "      <th>87_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.90</td>\n",
       "      <td>11.2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>9.6</td>\n",
       "      <td>30.40</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.9</td>\n",
       "      <td>35.90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>8.4</td>\n",
       "      <td>14.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flag  discharge_location  51006  51277   781   age  50868  644  50912  432  \\\n",
       "0   1.0                 4.0   69.0   14.0  49.0  85.0   15.0  0.0    1.9  0.0   \n",
       "1   0.0                 0.0   18.0   16.7  19.0  67.0   15.0  0.0    1.1  0.0   \n",
       "2   0.0                 1.0   18.0   16.3  15.0  67.0   12.0  0.0    1.0  4.0   \n",
       "3   1.0                 3.0   36.0   17.9  29.0  84.0   12.0  0.0    1.0  3.0   \n",
       "4   0.0                 1.0   15.0   13.0  16.0  58.0   13.0  0.0    1.0  0.0   \n",
       "\n",
       "   ...   787  51279  50893  51244  51222  50882  51249  198_y  50862  87_y  \n",
       "0  ...  24.0   3.46    9.3  21.90   11.2   31.0   34.8   15.0    3.9  18.0  \n",
       "1  ...  24.0   3.59    9.6  30.40   11.5   25.0   35.7   15.0    3.2  19.0  \n",
       "2  ...  24.0   3.79    7.9  35.90   10.0   28.0   33.8   13.0    3.2  15.0  \n",
       "3  ...  18.0   3.66    8.4  14.00   11.5   29.0   35.1   15.0    3.0  14.0  \n",
       "4  ...  30.0   4.51    8.7  14.29   14.3   27.0   35.1   15.0    3.2  21.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unstructure_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.concat([data_structure,data_unstructure_corr], axis=1)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>787</th>\n",
       "      <th>51279</th>\n",
       "      <th>50893</th>\n",
       "      <th>51244</th>\n",
       "      <th>51222</th>\n",
       "      <th>50882</th>\n",
       "      <th>51249</th>\n",
       "      <th>198_y</th>\n",
       "      <th>50862</th>\n",
       "      <th>87_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.181246</td>\n",
       "      <td>-0.171979</td>\n",
       "      <td>-0.020658</td>\n",
       "      <td>-0.286978</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.082198</td>\n",
       "      <td>0.090863</td>\n",
       "      <td>0.136441</td>\n",
       "      <td>0.241859</td>\n",
       "      <td>0.211144</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.90</td>\n",
       "      <td>11.2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.177999</td>\n",
       "      <td>-0.171340</td>\n",
       "      <td>-0.018405</td>\n",
       "      <td>-0.279906</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>0.093037</td>\n",
       "      <td>0.130213</td>\n",
       "      <td>0.242734</td>\n",
       "      <td>0.204262</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>9.6</td>\n",
       "      <td>30.40</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170776</td>\n",
       "      <td>-0.169037</td>\n",
       "      <td>-0.019629</td>\n",
       "      <td>-0.270112</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.083863</td>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.128219</td>\n",
       "      <td>0.242143</td>\n",
       "      <td>0.208636</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.9</td>\n",
       "      <td>35.90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.186937</td>\n",
       "      <td>-0.182405</td>\n",
       "      <td>-0.026913</td>\n",
       "      <td>-0.296987</td>\n",
       "      <td>0.028211</td>\n",
       "      <td>0.082213</td>\n",
       "      <td>0.100136</td>\n",
       "      <td>0.137477</td>\n",
       "      <td>0.243451</td>\n",
       "      <td>0.205574</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>8.4</td>\n",
       "      <td>14.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.163789</td>\n",
       "      <td>-0.155432</td>\n",
       "      <td>-0.004861</td>\n",
       "      <td>-0.258247</td>\n",
       "      <td>-0.003251</td>\n",
       "      <td>0.074688</td>\n",
       "      <td>0.078321</td>\n",
       "      <td>0.120354</td>\n",
       "      <td>0.244487</td>\n",
       "      <td>0.214203</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5       6_x         7  \\\n",
       "0 -0.181246 -0.171979 -0.020658 -0.286978  0.018740  0.082198  0.090863   \n",
       "1 -0.177999 -0.171340 -0.018405 -0.279906  0.013936  0.076547  0.093037   \n",
       "2 -0.170776 -0.169037 -0.019629 -0.270112  0.014875  0.083863  0.088988   \n",
       "3 -0.186937 -0.182405 -0.026913 -0.296987  0.028211  0.082213  0.100136   \n",
       "4 -0.163789 -0.155432 -0.004861 -0.258247 -0.003251  0.074688  0.078321   \n",
       "\n",
       "          8         9        10  ...   787  51279  50893  51244  51222  50882  \\\n",
       "0  0.136441  0.241859  0.211144  ...  24.0   3.46    9.3  21.90   11.2   31.0   \n",
       "1  0.130213  0.242734  0.204262  ...  24.0   3.59    9.6  30.40   11.5   25.0   \n",
       "2  0.128219  0.242143  0.208636  ...  24.0   3.79    7.9  35.90   10.0   28.0   \n",
       "3  0.137477  0.243451  0.205574  ...  18.0   3.66    8.4  14.00   11.5   29.0   \n",
       "4  0.120354  0.244487  0.214203  ...  30.0   4.51    8.7  14.29   14.3   27.0   \n",
       "\n",
       "   51249  198_y  50862  87_y  \n",
       "0   34.8   15.0    3.9  18.0  \n",
       "1   35.7   15.0    3.2  19.0  \n",
       "2   33.8   13.0    3.2  15.0  \n",
       "3   35.1   15.0    3.0  14.0  \n",
       "4   35.1   15.0    3.2  21.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array(new_data.drop(labels=['flag'], axis=1)), np.array(new_data['flag'].apply(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 299)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 666, test_size = 0.33 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_structure, X_train_unstructure = X_train[:, 0:200], X_train[:, 200:]\n",
    "X_test_structure, X_test_unstructure = X_test[:, 0:200], X_test[:, 200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 99)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unstructure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_train_unstructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.92276773e+00, 2.96495837e+01, 1.48659144e+01, 2.94938530e+01,\n",
       "       7.04806202e+01, 1.41410910e+01, 2.63313236e-01, 1.53306919e+00,\n",
       "       1.36713178e+00, 3.60875969e+00, 5.07258111e-01, 1.08979529e+01,\n",
       "       9.71043296e+01, 3.71498421e+00, 1.47809044e+01, 1.38102785e+00,\n",
       "       3.07332759e+01, 1.32399265e+02, 7.39322194e+01, 4.51264387e+02,\n",
       "       1.47621390e+01, 5.10340798e+00, 1.01623313e+00, 1.75101924e-01,\n",
       "       6.49038185e-02, 1.26933026e+02, 1.08484065e-01, 1.22855843e+01,\n",
       "       1.95047947e+01, 1.22967844e+01, 3.93051967e-01, 1.22751163e+01,\n",
       "       3.59665547e+02, 7.77353287e+01, 1.52384170e+04, 1.14967557e+01,\n",
       "       3.27829744e+02, 1.44291631e+02, 1.38579437e+02, 1.38365418e+02,\n",
       "       1.38028814e+02, 9.43152455e-01, 4.15067471e+00, 1.92120385e+01,\n",
       "       7.31094746e+00, 1.35886882e+02, 1.77031295e+00, 2.01042205e+00,\n",
       "       1.38360034e+02, 4.16648866e+00, 1.22400287e+02, 2.90510393e+01,\n",
       "       1.01811944e+01, 2.18291398e+02, 1.04946196e+02, 4.31105627e+01,\n",
       "       2.73993115e+00, 5.56662590e+01, 1.45464278e+02, 5.73153747e+01,\n",
       "       4.36159231e+01, 1.40700924e+02, 5.63928682e+01, 8.23428079e-01,\n",
       "       1.45300534e+02, 1.17209483e+02, 1.54853959e+02, 9.91615389e+01,\n",
       "       3.11024235e+01, 3.14450359e+01, 6.02228309e+01, 1.15352719e+02,\n",
       "       3.19377954e+01, 5.73051392e+00, 6.11076947e+01, 1.57793982e+02,\n",
       "       1.60728699e+02, 8.91366638e+01, 8.39076946e+00, 3.59409130e+00,\n",
       "       8.34460237e+00, 1.59171932e+02, 1.15746942e+02, 5.61876543e+01,\n",
       "       1.08646914e+01, 3.22682458e+01, 7.98115705e+01, 1.06151220e+01,\n",
       "       5.80802756e+01, 2.40643239e+01, 3.61699110e+00, 8.53154177e+00,\n",
       "       1.44193081e+01, 1.09052570e+01, 2.52899167e+01, 3.38227735e+01,\n",
       "       1.29007034e+01, 3.20157910e+00, 1.49531036e+01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unstructure_standard = standard_scaler.transform(X_train_unstructure)\n",
    "X_test_unstructure_standard = standard_scaler.transform(X_test_unstructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 99)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unstructure_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716, 99)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_unstructure_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 200)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3483, 200)\n",
      "(3483, 99)\n",
      "(3483,)\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n"
     ]
    }
   ],
   "source": [
    "class DiseaseData:\n",
    "    def __init__(self, train_model=True, need_shuffle=True):\n",
    "        if train_model:\n",
    "            self._data_structure = X_train_structure # 文本\n",
    "            self._data_unstructure = X_train_unstructure_standard # 数值\n",
    "            self._labels = y_train\n",
    "        else:\n",
    "            self._data_structure = X_test_structure # 文本\n",
    "            self._data_unstructure = X_test_unstructure_standard # 数值\n",
    "            self._labels = y_test\n",
    "        print(self._data_structure.shape)\n",
    "        print(self._data_unstructure.shape)\n",
    "        print(self._labels.shape)\n",
    "\n",
    "        self._num_examples = self._data_structure.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def _shuffle_data(self):\n",
    "        # [0,1,2,3,4,5] -> [5,3,2,4,0,1]\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data_structure = self._data_structure[p]\n",
    "        self._data_unstructure = self._data_unstructure[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data_structure = self._data_structure[self._indicator: end_indicator]\n",
    "        batch_data_unstructure = self._data_unstructure[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data_structure, batch_data_unstructure, batch_labels\n",
    "\n",
    "train_data = DiseaseData(True, True)\n",
    "test_data = DiseaseData(False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-3\n",
    "\n",
    "# # x1 得到 hidden1_1 结构化数据 20 * 200\n",
    "# x1 = tf.placeholder(tf.float32, [None, X_train_structure.shape[1]])\n",
    "# hidden1_1 = tf.layers.dense(x1, 100, activation=tf.nn.relu)\n",
    "\n",
    "# # x2 得到 hidden1_2 非结构化数据 20 * 99\n",
    "# x2 = tf.placeholder(tf.float32, [None, X_train_unstructure_standard.shape[1]])\n",
    "# hidden1_2 = tf.layers.dense(x2, 50, activation=tf.nn.relu)\n",
    "\n",
    "# y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# # # hidden1_1 和 hidden1_2 拼接通过DNN得到20*10的输出 取均值后得到w 20*1\n",
    "# # hidden_concat = tf.concat([hidden1_1, hidden1_2], 1)\n",
    "# # hidden_concat_1 = tf.layers.dense(hidden_concat, 100, activation=tf.nn.relu)\n",
    "# # hidden_concat_2 = tf.layers.dense(hidden_concat_1, 100, activation=tf.nn.relu)\n",
    "# # hidden_concat_3 = tf.layers.dense(hidden_concat_2, 50, activation=tf.nn.relu)\n",
    "# # hidden_res = tf.layers.dense(hidden_concat_3, 10)\n",
    "# # w = tf.reduce_mean(hidden_res, axis=1)\n",
    "# # w = tf.reshape(w, (20, 1))\n",
    "# # # hidden1_1 = w * hidden1_1 + hidden1_1\n",
    "# # # 结果为20*100\n",
    "# # hidden1_1 = w * hidden1_1 + hidden1_1\n",
    "# # hidden1_2 = w * hidden1_2 + hidden1_2\n",
    "\n",
    "# # 结构化数据 200维\n",
    "# hidden2_1 = tf.layers.dense(hidden1_1, 50, activation=tf.nn.relu)\n",
    "# hidden3_1 = tf.layers.dense(hidden2_1, 20, activation=tf.nn.relu)\n",
    "# y_1 = tf.layers.dense(hidden3_1, 1)\n",
    "\n",
    "# # 非结构化数据 100维\n",
    "# hidden2_2 = tf.layers.dense(hidden1_2, 30, activation=tf.nn.relu)\n",
    "# hidden3_2 = tf.layers.dense(hidden2_2, 10, activation=tf.nn.relu)\n",
    "# y_2 = tf.layers.dense(hidden3_2, 1)\n",
    "\n",
    "# # 计算损失\n",
    "# # loss_1 = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_1)\n",
    "# # loss_2 = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_2)\n",
    "# p_y_1 = tf.nn.sigmoid(y_1)\n",
    "# p_y_2 = tf.nn.sigmoid(y_2)\n",
    "# y_reshaped = tf.reshape(y, (-1, 1))\n",
    "# loss_1 = tf.reduce_mean(tf.square(tf.cast(y_reshaped, tf.float32) - p_y_1))\n",
    "# loss_2 = tf.reduce_mean(tf.square(tf.cast(y_reshaped, tf.float32) - p_y_2))\n",
    "\n",
    "# # 计算准确率\n",
    "# accuracy_1 = tf.reduce_mean(tf.cast(tf.equal(tf.cast(p_y_1 > 0.5, tf.int64), y_reshaped), tf.float64))\n",
    "# accuracy_2 = tf.reduce_mean(tf.cast(tf.equal(tf.cast(p_y_2 > 0.5, tf.int64), y_reshaped), tf.float64))\n",
    "# # accuracy_1 = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_1, 1), y), tf.float64))\n",
    "# # accuracy_2 = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_2, 1), y), tf.float64))\n",
    "\n",
    "# # 选择合适的损失函数\n",
    "# loss = loss_1 * 0.5 + loss_2 * 0.5\n",
    "# accuracy = accuracy_1 * 0.5 + accuracy_2 * 0.5\n",
    "\n",
    "# with tf.name_scope('train_op'):\n",
    "#     train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "# x1 得到 hidden1_1 结构化数据 20 * 200\n",
    "x1 = tf.placeholder(tf.float32, [None, X_train_structure.shape[1]])\n",
    "hidden1_1 = tf.layers.dense(x1, 150, activation=tf.nn.relu)\n",
    "\n",
    "# x2 得到 hidden1_2 非结构化数据 20 * 99\n",
    "x2 = tf.placeholder(tf.float32, [None, X_train_unstructure_standard.shape[1]])\n",
    "hidden1_2 = tf.layers.dense(x2, 50, activation=tf.nn.relu)\n",
    "\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# hidden1_1 和 hidden1_2 拼接通过DNN得到20*10的输出 取均值后得到w 20*1\n",
    "hidden_concat = tf.concat([hidden1_1, hidden1_2], 1)\n",
    "hidden_concat_1 = tf.layers.dense(hidden_concat, 100, activation=tf.nn.relu)\n",
    "hidden_concat_2 = tf.layers.dense(hidden_concat_1, 100, activation=tf.nn.relu)\n",
    "hidden_concat_3 = tf.layers.dense(hidden_concat_2, 50, activation=tf.nn.relu)\n",
    "hidden_res = tf.layers.dense(hidden_concat_3, 10)\n",
    "w = tf.reduce_mean(hidden_res, axis=1)\n",
    "w = tf.reshape(w, (20, 1))\n",
    "# hidden1_1 = w * hidden1_1 + hidden1_1\n",
    "# 结果为20*100\n",
    "hidden1_1 = w * hidden1_1 + hidden1_1\n",
    "hidden1_2 = w * hidden1_2 + hidden1_2\n",
    "\n",
    "# 结构化数据 200维\n",
    "hidden2_1 = tf.layers.dense(hidden1_1, 80, activation=tf.nn.relu)\n",
    "hidden3_1 = tf.layers.dense(hidden2_1, 30, activation=tf.nn.relu)\n",
    "y_1 = tf.layers.dense(hidden3_1, 2)\n",
    "\n",
    "# 非结构化数据 100维\n",
    "hidden2_2 = tf.layers.dense(hidden1_2, 30, activation=tf.nn.relu)\n",
    "hidden3_2 = tf.layers.dense(hidden2_2, 10, activation=tf.nn.relu)\n",
    "y_2 = tf.layers.dense(hidden3_2, 2)\n",
    "\n",
    "# 计算损失\n",
    "loss_1 = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_1)\n",
    "loss_2 = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_2)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_1, 1), y), tf.float64))\n",
    "accuracy_2 = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_2, 1), y), tf.float64))\n",
    "\n",
    "# 选择合适的损失函数\n",
    "loss = loss_1 * 0.2 + loss_2 * 0.8\n",
    "accuracy = accuracy_1 * 0.2 + accuracy_2 * 0.8\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss: 0.29095, loss_1: 0.54002, loss_2: 0.22869, acc: 0.88000, acc_1: 0.80000, acc_2: 0.90000\n",
      "[Train] Step: 1000, loss: 0.34066, loss_1: 0.44769, loss_2: 0.31390, acc: 0.81000, acc_1: 0.85000, acc_2: 0.80000\n",
      "[Train] Step: 1500, loss: 0.20242, loss_1: 0.58631, loss_2: 0.10645, acc: 0.87000, acc_1: 0.75000, acc_2: 0.90000\n",
      "[Train] Step: 2000, loss: 0.34892, loss_1: 0.70150, loss_2: 0.26077, acc: 0.84000, acc_1: 0.60000, acc_2: 0.90000\n",
      "[Train] Step: 2500, loss: 0.22077, loss_1: 0.65674, loss_2: 0.11178, acc: 0.89000, acc_1: 0.65000, acc_2: 0.95000\n",
      "[Train] Step: 3000, loss: 0.14949, loss_1: 0.59147, loss_2: 0.03899, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 3500, loss: 0.12512, loss_1: 0.54600, loss_2: 0.01990, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 4000, loss: 0.13444, loss_1: 0.54424, loss_2: 0.03199, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 4500, loss: 0.11184, loss_1: 0.53575, loss_2: 0.00587, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 5000, loss: 0.15283, loss_1: 0.69238, loss_2: 0.01794, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 5000, acc: 0.76929, acc_1: 0.71706, acc_2: 0.78235\n",
      "[Train] Step: 5500, loss: 0.10336, loss_1: 0.51239, loss_2: 0.00110, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 6000, loss: 0.07924, loss_1: 0.39477, loss_2: 0.00036, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 6500, loss: 0.11077, loss_1: 0.55044, loss_2: 0.00085, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 7000, loss: 0.10588, loss_1: 0.52865, loss_2: 0.00019, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 7500, loss: 0.10944, loss_1: 0.54679, loss_2: 0.00010, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 8000, loss: 0.41470, loss_1: 0.45639, loss_2: 0.40428, acc: 0.91000, acc_1: 0.75000, acc_2: 0.95000\n",
      "[Train] Step: 8500, loss: 0.10653, loss_1: 0.42896, loss_2: 0.02593, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 9000, loss: 0.12843, loss_1: 0.63508, loss_2: 0.00177, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 9500, loss: 0.09342, loss_1: 0.45924, loss_2: 0.00197, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 10000, loss: 0.10447, loss_1: 0.52035, loss_2: 0.00051, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 10000, acc: 0.77012, acc_1: 0.73529, acc_2: 0.77882\n",
      "[Train] Step: 10500, loss: 0.09357, loss_1: 0.46749, loss_2: 0.00009, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 11000, loss: 0.08394, loss_1: 0.41686, loss_2: 0.00071, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 11500, loss: 0.14576, loss_1: 0.62935, loss_2: 0.02487, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 12000, loss: 0.12912, loss_1: 0.64524, loss_2: 0.00009, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 12500, loss: 0.07266, loss_1: 0.36259, loss_2: 0.00018, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 13000, loss: 0.08079, loss_1: 0.40349, loss_2: 0.00012, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 13500, loss: 0.12093, loss_1: 0.60346, loss_2: 0.00030, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 14000, loss: 0.11479, loss_1: 0.57369, loss_2: 0.00007, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 14500, loss: 0.09147, loss_1: 0.45707, loss_2: 0.00007, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 15000, loss: 0.14854, loss_1: 0.74256, loss_2: 0.00003, acc: 0.90000, acc_1: 0.50000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 15000, acc: 0.76882, acc_1: 0.73588, acc_2: 0.77706\n",
      "[Train] Step: 15500, loss: 0.04736, loss_1: 0.23654, loss_2: 0.00007, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 16000, loss: 0.05849, loss_1: 0.29245, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 16500, loss: 0.07712, loss_1: 0.38554, loss_2: 0.00001, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 17000, loss: 0.10385, loss_1: 0.51919, loss_2: 0.00001, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 17500, loss: 0.08313, loss_1: 0.41566, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 18000, loss: 0.11928, loss_1: 0.59638, loss_2: 0.00001, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 18500, loss: 0.07116, loss_1: 0.35579, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 19000, loss: 0.06184, loss_1: 0.30919, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 19500, loss: 0.10034, loss_1: 0.50169, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 20000, loss: 0.06035, loss_1: 0.30176, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 20000, acc: 0.76635, acc_1: 0.73529, acc_2: 0.77412\n",
      "[Train] Step: 20500, loss: 0.11095, loss_1: 0.55475, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 21000, loss: 0.09555, loss_1: 0.47776, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 21500, loss: 0.10223, loss_1: 0.51114, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 22000, loss: 0.10366, loss_1: 0.51829, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 22500, loss: 0.09970, loss_1: 0.49851, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 23000, loss: 0.18304, loss_1: 0.38743, loss_2: 0.13194, acc: 0.94000, acc_1: 0.90000, acc_2: 0.95000\n",
      "[Train] Step: 23500, loss: 0.16391, loss_1: 0.81399, loss_2: 0.00139, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 24000, loss: 0.07609, loss_1: 0.37779, loss_2: 0.00067, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 24500, loss: 0.08689, loss_1: 0.43264, loss_2: 0.00045, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 25000, loss: 0.14770, loss_1: 0.73594, loss_2: 0.00064, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 25000, acc: 0.75718, acc_1: 0.74118, acc_2: 0.76118\n",
      "[Train] Step: 25500, loss: 0.08956, loss_1: 0.44744, loss_2: 0.00009, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 26000, loss: 0.07609, loss_1: 0.38019, loss_2: 0.00007, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 26500, loss: 0.12074, loss_1: 0.60304, loss_2: 0.00016, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 27000, loss: 0.08832, loss_1: 0.44023, loss_2: 0.00035, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 27500, loss: 0.09005, loss_1: 0.45000, loss_2: 0.00006, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 28000, loss: 0.07884, loss_1: 0.39410, loss_2: 0.00002, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 28500, loss: 0.08230, loss_1: 0.41138, loss_2: 0.00003, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 29000, loss: 0.11421, loss_1: 0.57091, loss_2: 0.00004, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 29500, loss: 0.12212, loss_1: 0.61051, loss_2: 0.00002, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 30000, loss: 0.10659, loss_1: 0.53283, loss_2: 0.00003, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 30000, acc: 0.76376, acc_1: 0.74824, acc_2: 0.76765\n",
      "[Train] Step: 30500, loss: 0.06927, loss_1: 0.34624, loss_2: 0.00002, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 31000, loss: 0.09867, loss_1: 0.49328, loss_2: 0.00001, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 31500, loss: 0.06206, loss_1: 0.31029, loss_2: 0.00001, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 32000, loss: 0.09259, loss_1: 0.46290, loss_2: 0.00002, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 32500, loss: 0.13443, loss_1: 0.67207, loss_2: 0.00002, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 33000, loss: 0.10527, loss_1: 0.52635, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 33500, loss: 0.09409, loss_1: 0.47040, loss_2: 0.00001, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 34000, loss: 0.04371, loss_1: 0.21857, loss_2: 0.00000, acc: 1.00000, acc_1: 1.00000, acc_2: 1.00000\n",
      "[Train] Step: 34500, loss: 0.11800, loss_1: 0.58997, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 35000, loss: 0.10602, loss_1: 0.53009, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 35000, acc: 0.76459, acc_1: 0.75471, acc_2: 0.76706\n",
      "[Train] Step: 35500, loss: 0.13463, loss_1: 0.67316, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 36000, loss: 0.08464, loss_1: 0.42320, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 36500, loss: 0.07846, loss_1: 0.39229, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 37000, loss: 0.11843, loss_1: 0.59212, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 37500, loss: 0.10342, loss_1: 0.51712, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 38000, loss: 0.13870, loss_1: 0.69352, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 38500, loss: 0.09730, loss_1: 0.48650, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 39000, loss: 0.10204, loss_1: 0.51019, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 39500, loss: 0.15929, loss_1: 0.55210, loss_2: 0.06109, acc: 0.92000, acc_1: 0.80000, acc_2: 0.95000\n",
      "[Train] Step: 40000, loss: 0.08101, loss_1: 0.39959, loss_2: 0.00137, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 40000, acc: 0.75918, acc_1: 0.76059, acc_2: 0.75882\n",
      "[Train] Step: 40500, loss: 0.09865, loss_1: 0.49168, loss_2: 0.00039, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 41000, loss: 0.11406, loss_1: 0.56957, loss_2: 0.00018, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 41500, loss: 0.04322, loss_1: 0.21569, loss_2: 0.00010, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 42000, loss: 0.05536, loss_1: 0.27555, loss_2: 0.00031, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 42500, loss: 0.09103, loss_1: 0.45513, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 43000, loss: 0.09386, loss_1: 0.46875, loss_2: 0.00014, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 43500, loss: 0.10165, loss_1: 0.50807, loss_2: 0.00004, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 44000, loss: 0.09214, loss_1: 0.46061, loss_2: 0.00002, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 44500, loss: 0.10392, loss_1: 0.51934, loss_2: 0.00006, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 45000, loss: 0.10231, loss_1: 0.51126, loss_2: 0.00007, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 45000, acc: 0.75965, acc_1: 0.76529, acc_2: 0.75824\n",
      "[Train] Step: 45500, loss: 0.09099, loss_1: 0.45482, loss_2: 0.00004, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 46000, loss: 0.08626, loss_1: 0.43122, loss_2: 0.00002, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 46500, loss: 0.05118, loss_1: 0.25584, loss_2: 0.00002, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 47000, loss: 0.09925, loss_1: 0.49611, loss_2: 0.00003, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 47500, loss: 0.05936, loss_1: 0.29669, loss_2: 0.00002, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 48000, loss: 0.07579, loss_1: 0.37891, loss_2: 0.00001, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 48500, loss: 0.09456, loss_1: 0.47277, loss_2: 0.00001, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 49000, loss: 0.09086, loss_1: 0.45428, loss_2: 0.00001, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 49500, loss: 0.09964, loss_1: 0.49821, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 50000, loss: 0.09512, loss_1: 0.47561, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 50000, acc: 0.75894, acc_1: 0.74765, acc_2: 0.76176\n",
      "[Train] Step: 50500, loss: 0.10829, loss_1: 0.54142, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 51000, loss: 0.09589, loss_1: 0.47947, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 51500, loss: 0.08793, loss_1: 0.43967, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 52000, loss: 0.06764, loss_1: 0.33820, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 52500, loss: 0.14117, loss_1: 0.70585, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 53000, loss: 0.08431, loss_1: 0.42153, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 53500, loss: 0.07847, loss_1: 0.39235, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 54000, loss: 0.08652, loss_1: 0.43260, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 54500, loss: 0.09810, loss_1: 0.49052, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 55000, loss: 0.08941, loss_1: 0.44706, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 55000, acc: 0.75812, acc_1: 0.73412, acc_2: 0.76412\n",
      "[Train] Step: 55500, loss: 0.09024, loss_1: 0.45118, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 56000, loss: 0.12704, loss_1: 0.63520, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 56500, loss: 0.11171, loss_1: 0.55855, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 57000, loss: 0.06560, loss_1: 0.32799, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 57500, loss: 0.06758, loss_1: 0.33789, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 58000, loss: 0.08986, loss_1: 0.44929, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 58500, loss: 0.10706, loss_1: 0.53531, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 59000, loss: 0.17233, loss_1: 0.86165, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 59500, loss: 0.12289, loss_1: 0.61444, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 60000, loss: 0.06051, loss_1: 0.30253, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 60000, acc: 0.76153, acc_1: 0.74882, acc_2: 0.76471\n",
      "[Train] Step: 60500, loss: 0.08286, loss_1: 0.41432, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 61000, loss: 0.09172, loss_1: 0.45861, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 61500, loss: 0.05498, loss_1: 0.27492, loss_2: 0.00000, acc: 1.00000, acc_1: 1.00000, acc_2: 1.00000\n",
      "[Train] Step: 62000, loss: 0.08362, loss_1: 0.41809, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 62500, loss: 0.09088, loss_1: 0.45440, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 63000, loss: 0.06326, loss_1: 0.31630, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 63500, loss: 0.18060, loss_1: 0.90301, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 64000, loss: 0.08678, loss_1: 0.43389, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 64500, loss: 0.06390, loss_1: 0.31951, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 65000, loss: 0.08767, loss_1: 0.43834, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 65000, acc: 0.76918, acc_1: 0.77059, acc_2: 0.76882\n",
      "[Train] Step: 65500, loss: 0.11540, loss_1: 0.57702, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 66000, loss: 0.08467, loss_1: 0.42334, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 66500, loss: 0.07511, loss_1: 0.37553, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 67000, loss: 0.09420, loss_1: 0.47099, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 67500, loss: 0.10147, loss_1: 0.50733, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 68000, loss: 0.05039, loss_1: 0.25195, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 68500, loss: 0.14889, loss_1: 0.74446, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 69000, loss: 0.09621, loss_1: 0.48104, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 69500, loss: 0.06595, loss_1: 0.32976, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 70000, loss: 0.08738, loss_1: 0.43690, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 70000, acc: 0.77235, acc_1: 0.77000, acc_2: 0.77294\n",
      "[Train] Step: 70500, loss: 0.10896, loss_1: 0.54480, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 71000, loss: 0.08800, loss_1: 0.43999, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 71500, loss: 0.07348, loss_1: 0.36742, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 72000, loss: 0.10197, loss_1: 0.50984, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 72500, loss: 0.08535, loss_1: 0.42673, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 73000, loss: 0.08381, loss_1: 0.41906, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 73500, loss: 0.08881, loss_1: 0.44405, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 74000, loss: 0.10773, loss_1: 0.53865, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 74500, loss: 0.10536, loss_1: 0.52682, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 75000, loss: 0.08385, loss_1: 0.41925, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 75000, acc: 0.76976, acc_1: 0.76647, acc_2: 0.77059\n",
      "[Train] Step: 75500, loss: 0.04243, loss_1: 0.21213, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 76000, loss: 0.08268, loss_1: 0.41340, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 76500, loss: 0.09668, loss_1: 0.48338, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 77000, loss: 0.10886, loss_1: 0.54432, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 77500, loss: 0.07578, loss_1: 0.37892, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 78000, loss: 0.10465, loss_1: 0.52324, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 78500, loss: 0.10901, loss_1: 0.54503, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 79000, loss: 0.07897, loss_1: 0.39484, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 79500, loss: 0.11834, loss_1: 0.59171, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 80000, loss: 0.08166, loss_1: 0.40830, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 80000, acc: 0.76706, acc_1: 0.76471, acc_2: 0.76765\n",
      "[Train] Step: 80500, loss: 0.06935, loss_1: 0.34677, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 81000, loss: 0.11207, loss_1: 0.56035, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 81500, loss: 0.07616, loss_1: 0.38079, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 82000, loss: 0.10150, loss_1: 0.50750, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 82500, loss: 0.08757, loss_1: 0.43783, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 83000, loss: 0.14003, loss_1: 0.70015, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 83500, loss: 0.11057, loss_1: 0.55283, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 84000, loss: 0.07980, loss_1: 0.39899, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 84500, loss: 0.11156, loss_1: 0.55782, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 85000, loss: 0.06913, loss_1: 0.34564, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 85000, acc: 0.76624, acc_1: 0.76529, acc_2: 0.76647\n",
      "[Train] Step: 85500, loss: 0.10823, loss_1: 0.54116, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 86000, loss: 0.08528, loss_1: 0.42640, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 86500, loss: 0.09869, loss_1: 0.49345, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 87000, loss: 0.05898, loss_1: 0.29490, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 87500, loss: 0.05798, loss_1: 0.28988, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 88000, loss: 0.09692, loss_1: 0.48458, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 88500, loss: 0.07129, loss_1: 0.35643, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 89000, loss: 0.08888, loss_1: 0.44439, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 89500, loss: 0.09842, loss_1: 0.49210, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 90000, loss: 0.05901, loss_1: 0.29504, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 90000, acc: 0.76776, acc_1: 0.76824, acc_2: 0.76765\n",
      "[Train] Step: 90500, loss: 0.07438, loss_1: 0.37191, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 91000, loss: 0.07750, loss_1: 0.38752, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 91500, loss: 0.13048, loss_1: 0.65238, loss_2: 0.00000, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 92000, loss: 0.09240, loss_1: 0.46199, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 92500, loss: 0.09227, loss_1: 0.46137, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 93000, loss: 0.07824, loss_1: 0.39118, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 93500, loss: 0.08685, loss_1: 0.43426, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 94000, loss: 0.08160, loss_1: 0.40798, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 94500, loss: 0.07421, loss_1: 0.37104, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 95000, loss: 0.07714, loss_1: 0.38570, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 95000, acc: 0.76612, acc_1: 0.76000, acc_2: 0.76765\n",
      "[Train] Step: 95500, loss: 0.08730, loss_1: 0.43650, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 96000, loss: 0.08579, loss_1: 0.42897, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 96500, loss: 0.14591, loss_1: 0.72955, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 97000, loss: 0.09703, loss_1: 0.48517, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 97500, loss: 0.04911, loss_1: 0.24557, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 98000, loss: 0.07601, loss_1: 0.38004, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 98500, loss: 0.08658, loss_1: 0.43288, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 99000, loss: 0.10718, loss_1: 0.53588, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 99500, loss: 0.13083, loss_1: 0.65414, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 100000, loss: 0.06096, loss_1: 0.30478, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 100000, acc: 0.76494, acc_1: 0.75882, acc_2: 0.76647\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps = 85\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data_structure, batch_data_unstructure, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, loss_val_1, loss_val_2, acc_val, acc_val_1, acc_val_2,  _ = sess.run(\n",
    "            [loss, loss_1, loss_2, accuracy, accuracy_1, accuracy_2, train_op],\n",
    "            feed_dict={\n",
    "                x1: batch_data_structure, # 文本\n",
    "                x2: batch_data_unstructure, # 数值\n",
    "                y: batch_labels})\n",
    "        if (i+1) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss: %4.5f, loss_1: %4.5f, loss_2: %4.5f, acc: %4.5f, acc_1: %4.5f, acc_2: %4.5f' \n",
    "                  % (i+1, loss_val, loss_val_1, loss_val_2, acc_val, acc_val_1, acc_val_2))\n",
    "        if (i+1) % 5000 == 0:\n",
    "            test_data = DiseaseData(False, False)\n",
    "            all_test_acc_val = []\n",
    "            all_test_acc_val_1 = []\n",
    "            all_test_acc_val_2 = []\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data_structure, test_batch_data_unstructure, test_batch_labels \\\n",
    "                    = test_data.next_batch(batch_size)\n",
    "                test_acc_val, test_acc_val_1, test_acc_val_2 = sess.run(\n",
    "                    [accuracy, accuracy_1, accuracy_2],\n",
    "                    feed_dict = {\n",
    "                        x1: test_batch_data_structure, # 文本\n",
    "                        x2: test_batch_data_unstructure, # 数值\n",
    "                        y: test_batch_labels\n",
    "                    })\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "                all_test_acc_val_1.append(test_acc_val_1)\n",
    "                all_test_acc_val_2.append(test_acc_val_2)\n",
    "            print('[Test ] Step: %d, acc: %4.5f, acc_1: %4.5f, acc_2: %4.5f'\n",
    "                  % (i+1, np.mean(all_test_acc_val), np.mean(all_test_acc_val_1), np.mean(all_test_acc_val_2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6_cpu",
   "language": "python",
   "name": "py36_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

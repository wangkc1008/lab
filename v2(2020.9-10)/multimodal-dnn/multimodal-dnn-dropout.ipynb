{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\anaconda3\\envs\\py36_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 310)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/wangkc/Desktop/胡喜风预测论文/mixed_data_0717_drop_replace_fill_100_5079.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', '1', '2', '3', '4', '5', '6_x', '7', '8', '9', '10',\n",
       "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21',\n",
       "       '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
       "       '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43',\n",
       "       '44', '45', '46', '47', '48', '49', '50', '51_x', '52', '53', '54',\n",
       "       '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65',\n",
       "       '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76',\n",
       "       '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87_x',\n",
       "       '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98',\n",
       "       '99', '100', '101', '102', '103', '104', '105', '106', '107',\n",
       "       '108', '109', '110', '111', '112', '113', '114', '115', '116_x',\n",
       "       '117', '118', '119', '120', '121', '122', '123', '124', '125',\n",
       "       '126', '127', '128', '129', '130', '131', '132', '133', '134',\n",
       "       '135', '136', '137', '138', '139', '140', '141', '142', '143',\n",
       "       '144', '145', '146', '147', '148', '149', '150', '151', '152',\n",
       "       '153', '154', '155', '156', '157', '158', '159', '160', '161',\n",
       "       '162', '163', '164', '165', '166', '167', '168', '169', '170',\n",
       "       '171', '172', '173', '174', '175', '176', '177', '178', '179',\n",
       "       '180', '181', '182', '183', '184', '185', '186', '187', '188',\n",
       "       '189', '190', '191', '192', '193', '194', '195', '196', '197',\n",
       "       '198_x', '199', '200', 'Unnamed: 0.1', 'discharge_location',\n",
       "       'religion', 'marital_status', 'ethnicity', 'diagnosis', 'gender',\n",
       "       'descriptin', '644', '617', '8555', '220046', '220545', '618',\n",
       "       '430', '220546', '220603', '220624', '220645', '220050', '1522',\n",
       "       '220051', '220180', '220210', '220228', '212', '431', '432',\n",
       "       '1523', '1524', '1526', '1529', '455', '762', '470', '51_y',\n",
       "       '1535', '87_y', '1536', '1540', '1542', '198_y', '116_y', '781',\n",
       "       '8441', '8368', '225309', '8480', '225310', '225693', '226534',\n",
       "       '226537', '226540', '227429', '227457', '786', '787', '789', '793',\n",
       "       '806', '813', '814', '824', '827', '829', '833', '837', '850',\n",
       "       '851', '853', '861', '1087', '1127', '51249', '51274', '51277',\n",
       "       '51279', '51301', '50931', '50861', '50862', '50863', '50868',\n",
       "       '50878', '50882', '50893', '50912', '50954', '51006', '51221',\n",
       "       '51222', '51237', '50970', '51464', '51491', '50910', '51244',\n",
       "       '51256', '50911', '50963', '50909', '50903', '50904', '50905',\n",
       "       '50907', '51000', '50906', 'age', 'flag'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '3', '4', '5', '6_x', '7', '8', '9', '10', '11', '12',\n",
       "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',\n",
       "       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
       "       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
       "       '46', '47', '48', '49', '50', '51_x', '52', '53', '54', '55', '56',\n",
       "       '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67',\n",
       "       '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78',\n",
       "       '79', '80', '81', '82', '83', '84', '85', '86', '87_x', '88', '89',\n",
       "       '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100',\n",
       "       '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
       "       '110', '111', '112', '113', '114', '115', '116_x', '117', '118',\n",
       "       '119', '120', '121', '122', '123', '124', '125', '126', '127',\n",
       "       '128', '129', '130', '131', '132', '133', '134', '135', '136',\n",
       "       '137', '138', '139', '140', '141', '142', '143', '144', '145',\n",
       "       '146', '147', '148', '149', '150', '151', '152', '153', '154',\n",
       "       '155', '156', '157', '158', '159', '160', '161', '162', '163',\n",
       "       '164', '165', '166', '167', '168', '169', '170', '171', '172',\n",
       "       '173', '174', '175', '176', '177', '178', '179', '180', '181',\n",
       "       '182', '183', '184', '185', '186', '187', '188', '189', '190',\n",
       "       '191', '192', '193', '194', '195', '196', '197', '198_x', '199',\n",
       "       '200', 'discharge_location', 'religion', 'marital_status',\n",
       "       'ethnicity', 'diagnosis', 'gender', 'descriptin', '644', '617',\n",
       "       '8555', '220046', '220545', '618', '430', '220546', '220603',\n",
       "       '220624', '220645', '220050', '1522', '220051', '220180', '220210',\n",
       "       '220228', '212', '431', '432', '1523', '1524', '1526', '1529',\n",
       "       '455', '762', '470', '51_y', '1535', '87_y', '1536', '1540',\n",
       "       '1542', '198_y', '116_y', '781', '8441', '8368', '225309', '8480',\n",
       "       '225310', '225693', '226534', '226537', '226540', '227429',\n",
       "       '227457', '786', '787', '789', '793', '806', '813', '814', '824',\n",
       "       '827', '829', '833', '837', '850', '851', '853', '861', '1087',\n",
       "       '1127', '51249', '51274', '51277', '51279', '51301', '50931',\n",
       "       '50861', '50862', '50863', '50868', '50878', '50882', '50893',\n",
       "       '50912', '50954', '51006', '51221', '51222', '51237', '50970',\n",
       "       '51464', '51491', '50910', '51244', '51256', '50911', '50963',\n",
       "       '50909', '50903', '50904', '50905', '50907', '51000', '50906',\n",
       "       'age', 'flag'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(labels=['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 308)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure, data_unstructure = data.iloc[:, 0:200], data.iloc[:, 200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 108)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unstructure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = data_unstructure.corr()['flag'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.44188461,  0.34834065,  0.31095106,  0.29790541,\n",
       "        0.27268167,  0.2718151 ,  0.22905935,  0.22840715,  0.21215508,\n",
       "        0.20216547,  0.19953281,  0.18619697,  0.18439669,  0.16992431,\n",
       "        0.16440352,  0.16432356,  0.15335323,  0.14109696,  0.13949691,\n",
       "        0.12774348,  0.1271463 ,  0.11838982,  0.11299193,  0.1125567 ,\n",
       "        0.10861225,  0.1071288 ,  0.10025498,  0.09863866,  0.09458484,\n",
       "        0.09294859,  0.09262481,  0.09137232,  0.08903374,  0.08362927,\n",
       "        0.07543028,  0.07361162,  0.07360355,  0.0647498 ,  0.0627509 ,\n",
       "        0.05857573,  0.05222199,  0.05180129,  0.05133955,  0.04385232,\n",
       "        0.04211161,  0.03680885,  0.03597658,  0.03231726,  0.03117802,\n",
       "        0.02990743,  0.02118805,  0.01793353,  0.01481552,  0.00980294,\n",
       "        0.00526859,  0.0040515 ,  0.00249607,  0.00131689,  0.00101453,\n",
       "       -0.00442209, -0.00652561, -0.01491198, -0.0226915 , -0.0240744 ,\n",
       "       -0.02448223, -0.02635538, -0.02834546, -0.02946103, -0.03165345,\n",
       "       -0.03171733, -0.03414896, -0.03753092, -0.03795563, -0.03961062,\n",
       "       -0.04078438, -0.04597013, -0.05368642, -0.05778557, -0.05944319,\n",
       "       -0.06300886, -0.06446793, -0.06503558, -0.06854764, -0.06934461,\n",
       "       -0.07469666, -0.07549279, -0.08066237, -0.08301098, -0.08772217,\n",
       "       -0.09448277, -0.09965595, -0.10020189, -0.10905291, -0.11073511,\n",
       "       -0.11316227, -0.11405799, -0.12465224, -0.13460134, -0.13540862,\n",
       "       -0.15127792, -0.15433649, -0.16713406, -0.21699047, -0.23124315,\n",
       "       -0.25543088, -0.26503579, -0.32656055])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unstructure_corr = data_unstructure[corr_data[(corr_data > 0.01) | (corr_data < -0.01)].index]\n",
    "data_unstructure_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>51006</th>\n",
       "      <th>51277</th>\n",
       "      <th>781</th>\n",
       "      <th>age</th>\n",
       "      <th>50868</th>\n",
       "      <th>644</th>\n",
       "      <th>50912</th>\n",
       "      <th>432</th>\n",
       "      <th>...</th>\n",
       "      <th>787</th>\n",
       "      <th>51279</th>\n",
       "      <th>50893</th>\n",
       "      <th>51244</th>\n",
       "      <th>51222</th>\n",
       "      <th>50882</th>\n",
       "      <th>51249</th>\n",
       "      <th>198_y</th>\n",
       "      <th>50862</th>\n",
       "      <th>87_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.90</td>\n",
       "      <td>11.2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>9.6</td>\n",
       "      <td>30.40</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.9</td>\n",
       "      <td>35.90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>8.4</td>\n",
       "      <td>14.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flag  discharge_location  51006  51277   781   age  50868  644  50912  432  \\\n",
       "0   1.0                 4.0   69.0   14.0  49.0  85.0   15.0  0.0    1.9  0.0   \n",
       "1   0.0                 0.0   18.0   16.7  19.0  67.0   15.0  0.0    1.1  0.0   \n",
       "2   0.0                 1.0   18.0   16.3  15.0  67.0   12.0  0.0    1.0  4.0   \n",
       "3   1.0                 3.0   36.0   17.9  29.0  84.0   12.0  0.0    1.0  3.0   \n",
       "4   0.0                 1.0   15.0   13.0  16.0  58.0   13.0  0.0    1.0  0.0   \n",
       "\n",
       "   ...   787  51279  50893  51244  51222  50882  51249  198_y  50862  87_y  \n",
       "0  ...  24.0   3.46    9.3  21.90   11.2   31.0   34.8   15.0    3.9  18.0  \n",
       "1  ...  24.0   3.59    9.6  30.40   11.5   25.0   35.7   15.0    3.2  19.0  \n",
       "2  ...  24.0   3.79    7.9  35.90   10.0   28.0   33.8   13.0    3.2  15.0  \n",
       "3  ...  18.0   3.66    8.4  14.00   11.5   29.0   35.1   15.0    3.0  14.0  \n",
       "4  ...  30.0   4.51    8.7  14.29   14.3   27.0   35.1   15.0    3.2  21.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unstructure_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.concat([data_structure,data_unstructure_corr], axis=1)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>787</th>\n",
       "      <th>51279</th>\n",
       "      <th>50893</th>\n",
       "      <th>51244</th>\n",
       "      <th>51222</th>\n",
       "      <th>50882</th>\n",
       "      <th>51249</th>\n",
       "      <th>198_y</th>\n",
       "      <th>50862</th>\n",
       "      <th>87_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.181246</td>\n",
       "      <td>-0.171979</td>\n",
       "      <td>-0.020658</td>\n",
       "      <td>-0.286978</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.082198</td>\n",
       "      <td>0.090863</td>\n",
       "      <td>0.136441</td>\n",
       "      <td>0.241859</td>\n",
       "      <td>0.211144</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.90</td>\n",
       "      <td>11.2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.177999</td>\n",
       "      <td>-0.171340</td>\n",
       "      <td>-0.018405</td>\n",
       "      <td>-0.279906</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>0.093037</td>\n",
       "      <td>0.130213</td>\n",
       "      <td>0.242734</td>\n",
       "      <td>0.204262</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>9.6</td>\n",
       "      <td>30.40</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170776</td>\n",
       "      <td>-0.169037</td>\n",
       "      <td>-0.019629</td>\n",
       "      <td>-0.270112</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.083863</td>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.128219</td>\n",
       "      <td>0.242143</td>\n",
       "      <td>0.208636</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.9</td>\n",
       "      <td>35.90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.186937</td>\n",
       "      <td>-0.182405</td>\n",
       "      <td>-0.026913</td>\n",
       "      <td>-0.296987</td>\n",
       "      <td>0.028211</td>\n",
       "      <td>0.082213</td>\n",
       "      <td>0.100136</td>\n",
       "      <td>0.137477</td>\n",
       "      <td>0.243451</td>\n",
       "      <td>0.205574</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>8.4</td>\n",
       "      <td>14.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.163789</td>\n",
       "      <td>-0.155432</td>\n",
       "      <td>-0.004861</td>\n",
       "      <td>-0.258247</td>\n",
       "      <td>-0.003251</td>\n",
       "      <td>0.074688</td>\n",
       "      <td>0.078321</td>\n",
       "      <td>0.120354</td>\n",
       "      <td>0.244487</td>\n",
       "      <td>0.214203</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5       6_x         7  \\\n",
       "0 -0.181246 -0.171979 -0.020658 -0.286978  0.018740  0.082198  0.090863   \n",
       "1 -0.177999 -0.171340 -0.018405 -0.279906  0.013936  0.076547  0.093037   \n",
       "2 -0.170776 -0.169037 -0.019629 -0.270112  0.014875  0.083863  0.088988   \n",
       "3 -0.186937 -0.182405 -0.026913 -0.296987  0.028211  0.082213  0.100136   \n",
       "4 -0.163789 -0.155432 -0.004861 -0.258247 -0.003251  0.074688  0.078321   \n",
       "\n",
       "          8         9        10  ...   787  51279  50893  51244  51222  50882  \\\n",
       "0  0.136441  0.241859  0.211144  ...  24.0   3.46    9.3  21.90   11.2   31.0   \n",
       "1  0.130213  0.242734  0.204262  ...  24.0   3.59    9.6  30.40   11.5   25.0   \n",
       "2  0.128219  0.242143  0.208636  ...  24.0   3.79    7.9  35.90   10.0   28.0   \n",
       "3  0.137477  0.243451  0.205574  ...  18.0   3.66    8.4  14.00   11.5   29.0   \n",
       "4  0.120354  0.244487  0.214203  ...  30.0   4.51    8.7  14.29   14.3   27.0   \n",
       "\n",
       "   51249  198_y  50862  87_y  \n",
       "0   34.8   15.0    3.9  18.0  \n",
       "1   35.7   15.0    3.2  19.0  \n",
       "2   33.8   13.0    3.2  15.0  \n",
       "3   35.1   15.0    3.0  14.0  \n",
       "4   35.1   15.0    3.2  21.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array(new_data.drop(labels=['flag'], axis=1)), np.array(new_data['flag'].apply(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 299)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666,test_size = 0.33 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_structure, X_train_unstructure = X_train[:, 0:200], X_train[:, 200:]\n",
    "X_test_structure, X_test_unstructure = X_test[:, 0:200], X_test[:, 200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 99)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unstructure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_train_unstructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.92276773e+00, 2.96495837e+01, 1.48659144e+01, 2.94938530e+01,\n",
       "       7.04806202e+01, 1.41410910e+01, 2.63313236e-01, 1.53306919e+00,\n",
       "       1.36713178e+00, 3.60875969e+00, 5.07258111e-01, 1.08979529e+01,\n",
       "       9.71043296e+01, 3.71498421e+00, 1.47809044e+01, 1.38102785e+00,\n",
       "       3.07332759e+01, 1.32399265e+02, 7.39322194e+01, 4.51264387e+02,\n",
       "       1.47621390e+01, 5.10340798e+00, 1.01623313e+00, 1.75101924e-01,\n",
       "       6.49038185e-02, 1.26933026e+02, 1.08484065e-01, 1.22855843e+01,\n",
       "       1.95047947e+01, 1.22967844e+01, 3.93051967e-01, 1.22751163e+01,\n",
       "       3.59665547e+02, 7.77353287e+01, 1.52384170e+04, 1.14967557e+01,\n",
       "       3.27829744e+02, 1.44291631e+02, 1.38579437e+02, 1.38365418e+02,\n",
       "       1.38028814e+02, 9.43152455e-01, 4.15067471e+00, 1.92120385e+01,\n",
       "       7.31094746e+00, 1.35886882e+02, 1.77031295e+00, 2.01042205e+00,\n",
       "       1.38360034e+02, 4.16648866e+00, 1.22400287e+02, 2.90510393e+01,\n",
       "       1.01811944e+01, 2.18291398e+02, 1.04946196e+02, 4.31105627e+01,\n",
       "       2.73993115e+00, 5.56662590e+01, 1.45464278e+02, 5.73153747e+01,\n",
       "       4.36159231e+01, 1.40700924e+02, 5.63928682e+01, 8.23428079e-01,\n",
       "       1.45300534e+02, 1.17209483e+02, 1.54853959e+02, 9.91615389e+01,\n",
       "       3.11024235e+01, 3.14450359e+01, 6.02228309e+01, 1.15352719e+02,\n",
       "       3.19377954e+01, 5.73051392e+00, 6.11076947e+01, 1.57793982e+02,\n",
       "       1.60728699e+02, 8.91366638e+01, 8.39076946e+00, 3.59409130e+00,\n",
       "       8.34460237e+00, 1.59171932e+02, 1.15746942e+02, 5.61876543e+01,\n",
       "       1.08646914e+01, 3.22682458e+01, 7.98115705e+01, 1.06151220e+01,\n",
       "       5.80802756e+01, 2.40643239e+01, 3.61699110e+00, 8.53154177e+00,\n",
       "       1.44193081e+01, 1.09052570e+01, 2.52899167e+01, 3.38227735e+01,\n",
       "       1.29007034e+01, 3.20157910e+00, 1.49531036e+01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unstructure_standard = standard_scaler.transform(X_train_unstructure)\n",
    "X_test_unstructure_standard = standard_scaler.transform(X_test_unstructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 99)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unstructure_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716, 99)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_unstructure_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483, 200)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_structure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3483,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3483, 200)\n",
      "(3483, 99)\n",
      "(3483,)\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n"
     ]
    }
   ],
   "source": [
    "class DiseaseData:\n",
    "    def __init__(self, train_model=True, need_shuffle=True):\n",
    "        if train_model:\n",
    "            self._data_structure = X_train_structure\n",
    "            self._data_unstructure = X_train_unstructure_standard\n",
    "            self._labels = y_train\n",
    "        else:\n",
    "            self._data_structure = X_test_structure\n",
    "            self._data_unstructure = X_test_unstructure_standard\n",
    "            self._labels = y_test\n",
    "        print(self._data_structure.shape)\n",
    "        print(self._data_unstructure.shape)\n",
    "        print(self._labels.shape)\n",
    "\n",
    "        self._num_examples = self._data_structure.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def _shuffle_data(self):\n",
    "        # [0,1,2,3,4,5] -> [5,3,2,4,0,1]\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data_structure = self._data_structure[p]\n",
    "        self._data_unstructure = self._data_unstructure[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data_structure = self._data_structure[self._indicator: end_indicator]\n",
    "        batch_data_unstructure = self._data_unstructure[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data_structure, batch_data_unstructure, batch_labels\n",
    "\n",
    "train_data = DiseaseData(True, True)\n",
    "test_data = DiseaseData(False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# x1 得到 hidden1_1 结构化数据\n",
    "x1 = tf.placeholder(tf.float32, [None, X_train_structure.shape[1]])\n",
    "# 在输入层对输入数据先进行Dropout，便于应用到隐藏层\n",
    "x1_drop = tf.layers.dropout(x1, dropout_rate)\n",
    "hidden1_1 = tf.layers.dense(x1_drop, 150, activation=tf.nn.relu)\n",
    "\n",
    "# x2 得到 hidden1_2 非结构化数据\n",
    "x2 = tf.placeholder(tf.float32, [None, X_train_unstructure_standard.shape[1]])\n",
    "x2_drop = tf.layers.dropout(x2, dropout_rate)\n",
    "hidden1_2 = tf.layers.dense(x2_drop, 50, activation=tf.nn.relu)\n",
    "\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# # hidden1_1 和 hidden1_2 拼接通过DNN得到20*10的输出 取均值后得到w 20*1\n",
    "# hidden_concat = tf.concat([hidden1_1, hidden1_2], 1)\n",
    "# hidden_concat_1 = tf.layers.dense(hidden_concat, 100, activation=tf.nn.relu)\n",
    "# hidden_concat_2 = tf.layers.dense(hidden_concat_1, 100, activation=tf.nn.relu)\n",
    "# hidden_concat_3 = tf.layers.dense(hidden_concat_2, 50, activation=tf.nn.relu)\n",
    "# hidden_res = tf.layers.dense(hidden_concat_3, 10)\n",
    "# w = tf.reduce_mean(hidden_res, axis=1)\n",
    "# w = tf.reshape(w, (20, 1))\n",
    "# # hidden1_1 = w * hidden1_1 + hidden1_1\n",
    "# # 结果为20*100\n",
    "# hidden1_1 = w * hidden1_1 + hidden1_1\n",
    "# hidden1_2 = w * hidden1_2 + hidden1_2\n",
    "\n",
    "# hidden1_1 hidden1_2 继续通过DNN......待定\n",
    "hidden2_1 = tf.layers.dense(hidden1_1, 80, activation=tf.nn.relu)\n",
    "hidden2_1_drop = tf.layers.dropout(hidden2_1, dropout_rate)\n",
    "hidden3_1 = tf.layers.dense(hidden2_1_drop, 30, activation=tf.nn.relu)\n",
    "hidden3_1_drop = tf.layers.dropout(hidden3_1, dropout_rate)\n",
    "y_1 = tf.layers.dense(hidden3_1_drop, 2)\n",
    "\n",
    "hidden2_2 = tf.layers.dense(hidden1_2, 30, activation=tf.nn.relu)\n",
    "hidden2_2_drop = tf.layers.dropout(hidden2_2, dropout_rate)\n",
    "hidden3_2 = tf.layers.dense(hidden2_2_drop, 10, activation=tf.nn.relu)\n",
    "hidden3_2_drop = tf.layers.dropout(hidden3_2, dropout_rate)\n",
    "y_2 = tf.layers.dense(hidden3_2_drop, 2)\n",
    "\n",
    "# 计算损失\n",
    "loss_1 = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_1)\n",
    "loss_2 = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_2)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_1, 1), y), tf.float64))\n",
    "accuracy_2 = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_2, 1), y), tf.float64))\n",
    "\n",
    "# 选择合适的损失函数\n",
    "loss = loss_1 * 0.2 + loss_2 * 0.8\n",
    "accuracy = accuracy_1 * 0.2 + accuracy_2 * 0.8\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss: 0.38626, loss_1: 0.56599, loss_2: 0.34133, acc: 0.83000, acc_1: 0.75000, acc_2: 0.85000\n",
      "[Train] Step: 1000, loss: 0.32572, loss_1: 0.66866, loss_2: 0.23998, acc: 0.81000, acc_1: 0.65000, acc_2: 0.85000\n",
      "[Train] Step: 1500, loss: 0.36979, loss_1: 0.85835, loss_2: 0.24765, acc: 0.84000, acc_1: 0.40000, acc_2: 0.95000\n",
      "[Train] Step: 2000, loss: 0.17525, loss_1: 0.56510, loss_2: 0.07779, acc: 0.91000, acc_1: 0.75000, acc_2: 0.95000\n",
      "[Train] Step: 2500, loss: 0.15978, loss_1: 0.54921, loss_2: 0.06242, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 3000, loss: 0.11865, loss_1: 0.57778, loss_2: 0.00387, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 3500, loss: 0.13407, loss_1: 0.64240, loss_2: 0.00698, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 4000, loss: 0.08588, loss_1: 0.41805, loss_2: 0.00284, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 4500, loss: 0.12883, loss_1: 0.62858, loss_2: 0.00389, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 5000, loss: 0.12949, loss_1: 0.64566, loss_2: 0.00045, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 5000, acc: 0.76529, acc_1: 0.71353, acc_2: 0.77824\n",
      "[Train] Step: 5500, loss: 0.11399, loss_1: 0.56664, loss_2: 0.00083, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 6000, loss: 0.11735, loss_1: 0.58361, loss_2: 0.00078, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 6500, loss: 0.12943, loss_1: 0.64584, loss_2: 0.00033, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 7000, loss: 0.10813, loss_1: 0.53979, loss_2: 0.00022, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 7500, loss: 0.06631, loss_1: 0.33118, loss_2: 0.00009, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 8000, loss: 0.11202, loss_1: 0.55991, loss_2: 0.00005, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 8500, loss: 0.10209, loss_1: 0.51030, loss_2: 0.00003, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 9000, loss: 0.10691, loss_1: 0.53436, loss_2: 0.00005, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 9500, loss: 0.11633, loss_1: 0.58145, loss_2: 0.00005, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 10000, loss: 0.13963, loss_1: 0.69810, loss_2: 0.00001, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 10000, acc: 0.77059, acc_1: 0.73765, acc_2: 0.77882\n",
      "[Train] Step: 10500, loss: 0.11030, loss_1: 0.55147, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 11000, loss: 0.10385, loss_1: 0.51921, loss_2: 0.00002, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 11500, loss: 0.08385, loss_1: 0.41916, loss_2: 0.00002, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 12000, loss: 0.08809, loss_1: 0.44043, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 12500, loss: 0.08616, loss_1: 0.43077, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 13000, loss: 0.09517, loss_1: 0.47582, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 13500, loss: 0.06328, loss_1: 0.31638, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 14000, loss: 0.18146, loss_1: 0.90729, loss_2: 0.00000, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 14500, loss: 0.08793, loss_1: 0.43965, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 15000, loss: 0.05675, loss_1: 0.28374, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 15000, acc: 0.77035, acc_1: 0.72941, acc_2: 0.78059\n",
      "[Train] Step: 15500, loss: 0.09462, loss_1: 0.47311, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 16000, loss: 0.11696, loss_1: 0.58480, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 16500, loss: 0.09788, loss_1: 0.48940, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 17000, loss: 0.09655, loss_1: 0.48277, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 17500, loss: 0.08335, loss_1: 0.41676, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 18000, loss: 0.12461, loss_1: 0.62306, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 18500, loss: 0.07688, loss_1: 0.38438, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 19000, loss: 0.06435, loss_1: 0.32173, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 19500, loss: 0.12013, loss_1: 0.60063, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 20000, loss: 0.11325, loss_1: 0.56625, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 20000, acc: 0.77141, acc_1: 0.73471, acc_2: 0.78059\n",
      "[Train] Step: 20500, loss: 0.11397, loss_1: 0.56983, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 21000, loss: 0.08346, loss_1: 0.41729, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 21500, loss: 0.10731, loss_1: 0.53653, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 22000, loss: 0.11539, loss_1: 0.57696, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 22500, loss: 0.08802, loss_1: 0.44012, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 23000, loss: 0.11796, loss_1: 0.58978, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 23500, loss: 0.13049, loss_1: 0.65244, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 24000, loss: 0.06539, loss_1: 0.32697, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 24500, loss: 0.10123, loss_1: 0.50613, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 25000, loss: 0.08834, loss_1: 0.44169, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 25000, acc: 0.76941, acc_1: 0.73176, acc_2: 0.77882\n",
      "[Train] Step: 25500, loss: 0.06919, loss_1: 0.34597, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 26000, loss: 0.09798, loss_1: 0.48988, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 26500, loss: 0.09996, loss_1: 0.49979, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 27000, loss: 0.12869, loss_1: 0.64347, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 27500, loss: 0.10868, loss_1: 0.54340, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 28000, loss: 0.06582, loss_1: 0.32911, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 28500, loss: 0.08582, loss_1: 0.42912, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 29000, loss: 0.05893, loss_1: 0.29463, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 29500, loss: 0.11372, loss_1: 0.56862, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 30000, loss: 0.12296, loss_1: 0.61481, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 30000, acc: 0.77318, acc_1: 0.75529, acc_2: 0.77765\n",
      "[Train] Step: 30500, loss: 0.10899, loss_1: 0.54497, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 31000, loss: 0.05174, loss_1: 0.25870, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 31500, loss: 0.11541, loss_1: 0.57703, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 32000, loss: 0.06915, loss_1: 0.34575, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 32500, loss: 0.04350, loss_1: 0.21752, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 33000, loss: 0.08490, loss_1: 0.42452, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 33500, loss: 0.09140, loss_1: 0.45698, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 34000, loss: 0.12100, loss_1: 0.60502, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 34500, loss: 0.09554, loss_1: 0.47770, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 35000, loss: 0.07633, loss_1: 0.38165, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 35000, acc: 0.77082, acc_1: 0.75059, acc_2: 0.77588\n",
      "[Train] Step: 35500, loss: 0.08651, loss_1: 0.43256, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 36000, loss: 0.12661, loss_1: 0.63303, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 36500, loss: 0.09123, loss_1: 0.45614, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 37000, loss: 0.05977, loss_1: 0.29887, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 37500, loss: 0.08254, loss_1: 0.41272, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 38000, loss: 0.12450, loss_1: 0.62251, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 38500, loss: 0.08948, loss_1: 0.44738, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 39000, loss: 0.08293, loss_1: 0.41465, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 39500, loss: 0.08338, loss_1: 0.41691, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 40000, loss: 0.07359, loss_1: 0.36797, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 40000, acc: 0.77600, acc_1: 0.76471, acc_2: 0.77882\n",
      "[Train] Step: 40500, loss: 0.11137, loss_1: 0.55684, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 41000, loss: 0.08494, loss_1: 0.42468, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 41500, loss: 0.09479, loss_1: 0.47396, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 42000, loss: 0.06366, loss_1: 0.31830, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 42500, loss: 0.10590, loss_1: 0.52949, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 43000, loss: 0.13043, loss_1: 0.65213, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 43500, loss: 0.09425, loss_1: 0.47126, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 44000, loss: 0.09161, loss_1: 0.45807, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 44500, loss: 0.06822, loss_1: 0.34112, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 45000, loss: 0.08761, loss_1: 0.43806, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 45000, acc: 0.77612, acc_1: 0.76765, acc_2: 0.77824\n",
      "[Train] Step: 45500, loss: 0.08331, loss_1: 0.41655, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 46000, loss: 0.08460, loss_1: 0.42298, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 46500, loss: 0.11539, loss_1: 0.57696, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 47000, loss: 0.09436, loss_1: 0.47181, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 47500, loss: 0.08793, loss_1: 0.43967, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 48000, loss: 0.06883, loss_1: 0.34414, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 48500, loss: 0.08074, loss_1: 0.40371, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 49000, loss: 0.04403, loss_1: 0.22015, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 49500, loss: 0.09988, loss_1: 0.49938, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 50000, loss: 0.11992, loss_1: 0.59962, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 50000, acc: 0.77353, acc_1: 0.76176, acc_2: 0.77647\n",
      "[Train] Step: 50500, loss: 0.10664, loss_1: 0.53320, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 51000, loss: 0.10095, loss_1: 0.50477, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 51500, loss: 0.12187, loss_1: 0.60934, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 52000, loss: 0.12039, loss_1: 0.60195, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 52500, loss: 0.12702, loss_1: 0.63511, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 53000, loss: 0.08314, loss_1: 0.41571, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 53500, loss: 0.07094, loss_1: 0.35471, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 54000, loss: 0.13946, loss_1: 0.69732, loss_2: 0.00000, acc: 0.92000, acc_1: 0.60000, acc_2: 1.00000\n",
      "[Train] Step: 54500, loss: 0.08875, loss_1: 0.44375, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 55000, loss: 0.03585, loss_1: 0.17927, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 55000, acc: 0.77188, acc_1: 0.75353, acc_2: 0.77647\n",
      "[Train] Step: 55500, loss: 0.06240, loss_1: 0.31199, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 56000, loss: 0.07905, loss_1: 0.39525, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 56500, loss: 0.09323, loss_1: 0.46614, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 57000, loss: 0.09326, loss_1: 0.46632, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 57500, loss: 0.11303, loss_1: 0.56517, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 58000, loss: 0.08574, loss_1: 0.42869, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 58500, loss: 0.08054, loss_1: 0.40269, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 59000, loss: 0.07410, loss_1: 0.37049, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 59500, loss: 0.20870, loss_1: 0.56152, loss_2: 0.12049, acc: 0.90000, acc_1: 0.70000, acc_2: 0.95000\n",
      "[Train] Step: 60000, loss: 0.10181, loss_1: 0.50827, loss_2: 0.00020, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 60000, acc: 0.76000, acc_1: 0.74353, acc_2: 0.76412\n",
      "[Train] Step: 60500, loss: 0.09599, loss_1: 0.47915, loss_2: 0.00020, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 61000, loss: 0.14743, loss_1: 0.73648, loss_2: 0.00017, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 61500, loss: 0.13262, loss_1: 0.66303, loss_2: 0.00001, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 62000, loss: 0.08402, loss_1: 0.42001, loss_2: 0.00002, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 62500, loss: 0.07457, loss_1: 0.37232, loss_2: 0.00013, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 63000, loss: 0.07256, loss_1: 0.36281, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 63500, loss: 0.09571, loss_1: 0.47853, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 64000, loss: 0.07333, loss_1: 0.36641, loss_2: 0.00006, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 64500, loss: 0.09676, loss_1: 0.48380, loss_2: 0.00001, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 65000, loss: 0.08117, loss_1: 0.40585, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 65000, acc: 0.76424, acc_1: 0.75294, acc_2: 0.76706\n",
      "[Train] Step: 65500, loss: 0.07839, loss_1: 0.39194, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 66000, loss: 0.06154, loss_1: 0.30766, loss_2: 0.00002, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 66500, loss: 0.08855, loss_1: 0.44270, loss_2: 0.00001, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 67000, loss: 0.06535, loss_1: 0.32668, loss_2: 0.00002, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 67500, loss: 0.12729, loss_1: 0.63641, loss_2: 0.00001, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 68000, loss: 0.15745, loss_1: 0.78726, loss_2: 0.00000, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 68500, loss: 0.10846, loss_1: 0.54229, loss_2: 0.00001, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 69000, loss: 0.07775, loss_1: 0.38876, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 69500, loss: 0.10869, loss_1: 0.54346, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 70000, loss: 0.09732, loss_1: 0.48659, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 70000, acc: 0.76847, acc_1: 0.76235, acc_2: 0.77000\n",
      "[Train] Step: 70500, loss: 0.08985, loss_1: 0.44926, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 71000, loss: 0.07604, loss_1: 0.38018, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 71500, loss: 0.07270, loss_1: 0.36351, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 72000, loss: 0.08115, loss_1: 0.40577, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 72500, loss: 0.09852, loss_1: 0.49260, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 73000, loss: 0.11500, loss_1: 0.57500, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 73500, loss: 0.09097, loss_1: 0.45487, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 74000, loss: 0.09101, loss_1: 0.45505, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 74500, loss: 0.18760, loss_1: 0.93797, loss_2: 0.00000, acc: 0.91000, acc_1: 0.55000, acc_2: 1.00000\n",
      "[Train] Step: 75000, loss: 0.11620, loss_1: 0.58102, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 75000, acc: 0.76929, acc_1: 0.76412, acc_2: 0.77059\n",
      "[Train] Step: 75500, loss: 0.10922, loss_1: 0.54610, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 76000, loss: 0.06855, loss_1: 0.34276, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 76500, loss: 0.09829, loss_1: 0.49147, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 77000, loss: 0.06082, loss_1: 0.30411, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 77500, loss: 0.09143, loss_1: 0.45713, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 78000, loss: 0.09197, loss_1: 0.45986, loss_2: 0.00000, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 78500, loss: 0.11656, loss_1: 0.58282, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 79000, loss: 0.07111, loss_1: 0.35557, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 79500, loss: 0.05716, loss_1: 0.28582, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 80000, loss: 0.07135, loss_1: 0.35675, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 80000, acc: 0.77024, acc_1: 0.75941, acc_2: 0.77294\n",
      "[Train] Step: 80500, loss: 0.08670, loss_1: 0.43350, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 81000, loss: 0.08407, loss_1: 0.42033, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 81500, loss: 0.09737, loss_1: 0.48687, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 82000, loss: 0.06947, loss_1: 0.34733, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 82500, loss: 0.08258, loss_1: 0.41291, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 83000, loss: 0.10776, loss_1: 0.53882, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 83500, loss: 0.09698, loss_1: 0.48489, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 84000, loss: 0.09718, loss_1: 0.48592, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 84500, loss: 0.05224, loss_1: 0.26122, loss_2: 0.00000, acc: 0.99000, acc_1: 0.95000, acc_2: 1.00000\n",
      "[Train] Step: 85000, loss: 0.09467, loss_1: 0.47333, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 85000, acc: 0.76835, acc_1: 0.75471, acc_2: 0.77176\n",
      "[Train] Step: 85500, loss: 0.07835, loss_1: 0.39175, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 86000, loss: 0.04853, loss_1: 0.24266, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 86500, loss: 0.06361, loss_1: 0.31701, loss_2: 0.00026, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 87000, loss: 0.06935, loss_1: 0.33876, loss_2: 0.00200, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 87500, loss: 0.06061, loss_1: 0.30278, loss_2: 0.00006, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 88000, loss: 0.09750, loss_1: 0.48721, loss_2: 0.00007, acc: 0.97000, acc_1: 0.85000, acc_2: 1.00000\n",
      "[Train] Step: 88500, loss: 0.11569, loss_1: 0.57706, loss_2: 0.00034, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 89000, loss: 0.06469, loss_1: 0.32345, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 89500, loss: 0.11600, loss_1: 0.58001, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 90000, loss: 0.13563, loss_1: 0.67806, loss_2: 0.00003, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 90000, acc: 0.76424, acc_1: 0.75059, acc_2: 0.76765\n",
      "[Train] Step: 90500, loss: 0.05136, loss_1: 0.25662, loss_2: 0.00005, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 91000, loss: 0.07358, loss_1: 0.36790, loss_2: 0.00000, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 91500, loss: 0.12048, loss_1: 0.60237, loss_2: 0.00000, acc: 0.94000, acc_1: 0.70000, acc_2: 1.00000\n",
      "[Train] Step: 92000, loss: 0.06593, loss_1: 0.32960, loss_2: 0.00002, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 92500, loss: 0.06656, loss_1: 0.33276, loss_2: 0.00001, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 93000, loss: 0.07751, loss_1: 0.38754, loss_2: 0.00001, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 93500, loss: 0.07601, loss_1: 0.38002, loss_2: 0.00001, acc: 0.98000, acc_1: 0.90000, acc_2: 1.00000\n",
      "[Train] Step: 94000, loss: 0.12152, loss_1: 0.60757, loss_2: 0.00001, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 94500, loss: 0.07963, loss_1: 0.39815, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 95000, loss: 0.07979, loss_1: 0.39892, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 95000, acc: 0.76859, acc_1: 0.76529, acc_2: 0.76941\n",
      "[Train] Step: 95500, loss: 0.10513, loss_1: 0.52563, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 96000, loss: 0.18022, loss_1: 0.90105, loss_2: 0.00001, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 96500, loss: 0.08002, loss_1: 0.40005, loss_2: 0.00001, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 97000, loss: 0.10093, loss_1: 0.50465, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 97500, loss: 0.08617, loss_1: 0.43084, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "[Train] Step: 98000, loss: 0.10517, loss_1: 0.52584, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 98500, loss: 0.11843, loss_1: 0.59214, loss_2: 0.00000, acc: 0.93000, acc_1: 0.65000, acc_2: 1.00000\n",
      "[Train] Step: 99000, loss: 0.10664, loss_1: 0.53318, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 99500, loss: 0.11165, loss_1: 0.55825, loss_2: 0.00000, acc: 0.95000, acc_1: 0.75000, acc_2: 1.00000\n",
      "[Train] Step: 100000, loss: 0.08177, loss_1: 0.40885, loss_2: 0.00000, acc: 0.96000, acc_1: 0.80000, acc_2: 1.00000\n",
      "(1716, 200)\n",
      "(1716, 99)\n",
      "(1716,)\n",
      "[Test ] Step: 100000, acc: 0.77412, acc_1: 0.76235, acc_2: 0.77706\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps = 85\n",
    "\n",
    "# train: 100k: 51.%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data_structure, batch_data_unstructure, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, loss_val_1, loss_val_2, acc_val, acc_val_1, acc_val_2,  _ = sess.run(\n",
    "            [loss, loss_1, loss_2, accuracy, accuracy_1, accuracy_2, train_op],\n",
    "            feed_dict={\n",
    "                x1: batch_data_structure,\n",
    "                x2: batch_data_unstructure,\n",
    "                y: batch_labels})\n",
    "        if (i+1) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss: %4.5f, loss_1: %4.5f, loss_2: %4.5f, acc: %4.5f, acc_1: %4.5f, acc_2: %4.5f' \n",
    "                  % (i+1, loss_val, loss_val_1, loss_val_2, acc_val, acc_val_1, acc_val_2))\n",
    "        if (i+1) % 5000 == 0:\n",
    "            test_data = DiseaseData(False, False)\n",
    "            all_test_acc_val = []\n",
    "            all_test_acc_val_1 = []\n",
    "            all_test_acc_val_2 = []\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data_structure, test_batch_data_unstructure, test_batch_labels \\\n",
    "                    = test_data.next_batch(batch_size)\n",
    "                test_acc_val, test_acc_val_1, test_acc_val_2 = sess.run(\n",
    "                    [accuracy, accuracy_1, accuracy_2],\n",
    "                    feed_dict = {\n",
    "                        x1: test_batch_data_structure, \n",
    "                        x2: test_batch_data_unstructure, \n",
    "                        y: test_batch_labels\n",
    "                    })\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "                all_test_acc_val_1.append(test_acc_val_1)\n",
    "                all_test_acc_val_2.append(test_acc_val_2)\n",
    "            print('[Test ] Step: %d, acc: %4.5f, acc_1: %4.5f, acc_2: %4.5f'\n",
    "                  % (i+1, np.mean(all_test_acc_val), np.mean(all_test_acc_val_1), np.mean(all_test_acc_val_2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6_cpu",
   "language": "python",
   "name": "py36_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

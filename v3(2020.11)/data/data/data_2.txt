假设词表的长度V为10000，隐层维度N为300。对每个词进行one-hot处理，都会被转换成一个10000长度的向量，一个位置为1，其余全为0。Embedding的目的是用一个长度为300的词向量代表这个长度为10000的one-hot向量，也就是该词语。Embedding处理后有非常非常多的优点，这里不多赘述。对模型进行训练后，得到输入层到隐层的权重向量矩阵，维度为10000*300。